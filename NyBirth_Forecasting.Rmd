---
title: "NYbirth - Project"
author: "Monica Iyer"
date: "24/04/2020"
output: 
  pdf_document:
    fig_height: 4
    fig_caption: true
    latex_engine: pdflatex
---

# Summary

Through this report, I assess the different models that can be used to forecast the growth of the City of New York based upon monthly average birth data provided in _NYBirth.csv_. I have used Polynomial Regression, Regularized Regression, Holt Winters Modeling Techniques and Box-Jenkins Models. In order to compare the predictive power and significance of each of the models, I have compared using the Residuals Plot Daignostics and Average Prediction Error _APE_ . The final model can be used to forecast the average number of births in New York City in 1960-1961. 

## Data Pre-processing

```{r,message=FALSE, warning=FALSE}
nybirth <- read.csv("NYBirth.csv") #eliminate the first column of months and corresponding year
                                   # from NYBirth  and save it as a csv
nybirth_ts <- ts(nybirth, frequency=12, start=1946, end=1959+11/12)

#divide into training and test sets
nybirth_train <- window(nybirth_ts, start= 1946, end=1956+11/12)
nybirth_test<- window(nybirth_ts, start=1957, end=1959+11/12)
log_train <- log(nybirth_train)
log_test <- log(nybirth_test)
#plots
color <- c(rep("black", 144), rep("blue",24))
plot(nybirth_ts, col=color, type='b', pch=16,xlim=c(1946, 1960),xlab="Year",
     ylab="Average Monthly Births", 
     main="Average Monthly Births in New York")

nybirthtrain_decompose <- decompose(nybirth_train, type="additive")
nybirthtest_decompose <- decompose(nybirth_test, type="additive")
plot(nybirthtrain_decompose)

```
Insights:

- There is an obvious growing trend
- The data is seasonal as well (frequency =12)
- Seems like the (trend + seasonal + random) component is equal to the observed data, so ${X_t = m_t + S_t + R_t}$
- There doesnt seem to be much of a difference between additive and multiplicative in terms of decomposition - so I stick with additive decomposition
- The non-stationarity due to increasing trend is okay for regression, but since there is non-constant variance, it has to be handled. 



```{r, echo=FALSE}
#use Fligner test to see if the variance is stable or not
pval=rep(-999,41)
alpha = seq(-2,2,by=0.1)
seg = rep(1:12,each=14)
max.i = length(alpha)
for(i in 1:max.i){
  if(alpha[i]==0){
    temp = log(nybirth_ts)
    temp.pval = fligner.test(temp , seg)$p.value
    pval[i] = temp.pval
  }
  else{
    temp = nybirth_ts^alpha[i]
    temp.pval = fligner.test(temp , seg)$p.value
    pval[i] = temp.pval
  }
}
plot(pval~alpha , pch=16)
alpha.optim = alpha[which.max(pval)]
abline(v=alpha.optim,col="red",lty=2)
text(0,0.4,paste0("alpha = ",as.character(alpha.optim)))

```

When observing the Fligner test above, we see that the p-value is the highest for alpha =1 which is $(nybirth)^1$ hence no transformation is required in this case since the data has constant variance. 

\newpage

# Model Selection and Evaluation

## Traditional polynomial regression models

- The regression model will have to account for seasonality in the data, hence I have used _nyb_seasonal_ and for the trend which is represented by _t1_ . 

```{r,message=FALSE, warning=FALSE}
nyb_seasonal <- as.vector(cycle(nybirth_train))
nyb_seasonal_test <- as.vector(cycle(nybirth_test))

t1<- as.vector(time(nybirth_train))
t2 <- t1^2
t3 <- t1^3
t4 <- t1^4

t_test<- as.vector(time(nybirth_test))
d2 <- data.frame(t1=t_test,t2=t_test^2, nyb_seasonal= nyb_seasonal_test)
d3 <- data.frame(t1=t_test, t2=t_test^2, t3=t_test^3, nyb_seasonal= nyb_seasonal_test)
d4 <- data.frame(t1=t_test,t2=t_test^2,t3=t_test^3,t4=t_test^4, nyb_seasonal= nyb_seasonal_test)


poly1 <- lm(nybirth_train~t1+t2+nyb_seasonal)
poly2 <- lm(nybirth_train~t1+t2+t3+nyb_seasonal)
poly3 <- lm(nybirth_train~t1+t2+t3+t4+nyb_seasonal)

pred1 <- predict(poly1, d2)
pred2 <- predict(poly2, d3)
pred3 <- predict(poly3, d4)
APE_C=c()
APE_C[1] = sum((pred1- nybirth_test)^2)/length(nybirth_test)
APE_C[2] = sum((pred2- nybirth_test)^2)/length(nybirth_test)
APE_C[3] = sum((pred3- nybirth_test)^2)/length(nybirth_test)
APE_C

```
The best polynomial regression model of the three seems to be the one with $X_t = t + t^2 + season + \epsilon_t$ with the regular training and testing data. However I have tested different variations of the model based on the best APE value obtained 
- with the seasonal component _nyb_seasonal_ 
- without it
- with the log transformed data 
- different polynomial degrees, etc.

I perform residual diagnostics on $X_t = t + t^2 + season + \epsilon_t$ to see if we can make statistically significant assumptions using the model 

```{r, warning=FALSE, message=FALSE}
resid_poly = poly1$residuals
fitted_poly = poly1$fitted.values
par(mfrow=c(2,2))
# residuals vs. fitted values
plot(resid_poly~fitted_poly , pch=12)
abline(h=0,lwd=2,lty=2)
#QQplot
#qqnorm(resid_model)
#qqline(resid_model)
library(car)
qqPlot(resid_poly , pch=12)
#Residuals plot
plot(resid_poly , pch=12)
#ACF plot
acf(resid_poly , pch=12)

#test constant variance
resid_seg <- rep(1:11, each=12)
fligner.test(resid_poly, resid_seg)
#test Normality 
shapiro.test(resid_poly)
#randomness test
library(lawstat)
runs.test(resid_poly)
library(randtests)
difference.sign.test(resid_poly)
```

Insights from the residuals plot:

- The residuals vs the fitted values indicates some pattern and so does the residuals time series plot. This suggests that the mean is not constant 0
- The p-value in the runs test (5.841e-05) is very small, which confirms evidence against randomness of residuals. In the ACF plot, more than 5% of the spikes cross the confidence interval, indicating that the residuals are in fact dependent. 
- With the Fligner test we see that the p-value (0.7088) is fairly large, hence there is no violation of the constant variance assumption. 
- From the QQ plot, we see that the upper right and lower left tails of the distribution deviates sligthly from the normal distribution, however the Shapiro test's p-value (0.1808) indicates that the residuals are also normal. 

From the Residuals diagnostics, we see that the constant mean 0 and independence rules have been violated. However, the residuals have constant variance and follow the rules of a normal distribution. Hence the model $X_t = t + t^2 +season+ \epsilon_t$ , cannot be used to make statistically significant assumptions. This model will not be considered as a valid candidate in the model selection process. 

\newpage

## Regularized regression model (LASSO, Ridge, Elastic Net)

### Ridge Regression

For this form of regularized regression, I choose an optimal $\lambda$ value by using _cv.glmnet_ and a range of possible lambda values. I then used the $\lambda$ and $\alpha =0$ to determine _fit_ridge_ . For this regression model, I also tried variations of the time factor _t_ as $t^{2}$ and $t^{3}$, however _t_bind_ worked best when it consisted of time _t_ and seaonal effect _month_ since this produced the lowest APE value. 

```{r,message=FALSE, warning=FALSE}
set.seed(1)
library(glmnet)
t <- as.vector(time(nybirth_train))
month <- as.factor(cycle(nybirth_train))
#t_2 <- t^2
#t_3 <- t^3
t_bind <- as.matrix(cbind(t, month))
target <- as.vector(nybirth_train)

fit_ridge <- glmnet(x=t_bind, y=target, alpha=0, standardize = FALSE, 
                    intercept = TRUE, family = 'gaussian')
lambdas <- 10^seq(10,-2,length=100)
cv_ridge <- cv.glmnet(x=t_bind, y=target,alpha=0, lambda=lambdas)
optimal_lambda_ridge <- cv_ridge$lambda.min
optimal_lambda_ridge
plot(cv_ridge)
dtest = as.matrix(cbind(time(nybirth_test),cycle(nybirth_test)))
colnames(dtest) <- c("t","month")
pred_ridge <- predict(fit_ridge, s=optimal_lambda_ridge, newx=dtest)
ape_ridge <- sum((pred_ridge- nybirth_test)^2)/length(nybirth_test)
ape_ridge
```

We find that the optimal lambda value in this case is `r optimal_lambda_ridge`. We use this optimal lambda value to build the Ridge regression model.


### Lasso Regression

We use the same _t_bind_ and _target_ factors defined for the Ridge Regression. In this case $\alpha =1$ and we obtain a new _optimal_lambda_lasso_ value that is used to fit the Lasso Regression model. 

```{r,message=FALSE, warning=FALSE}
cv_lasso <- cv.glmnet(x=t_bind, y=target,alpha=1, lambda=lambdas)
optimal_lambda_lasso <- cv_lasso$lambda.min
optimal_lambda_lasso
par(mfrow=c(1,2))
plot(cv_lasso)
fit_lasso <- glmnet(x=t_bind, y=target, alpha=1, standardize = FALSE, 
                    intercept = TRUE, family = 'gaussian')
pred_lasso <- predict(fit_lasso, s=optimal_lambda_lasso, newx=dtest)
ape_lasso <- sum((pred_lasso - nybirth_test)^2)/length(nybirth_test)
ape_lasso
```

We find that the optimal lambda value in this case is `r optimal_lambda_lasso`. We use this optimal lambda value to build the Lasso regression model.

### Elastic Net Regression

I choose a different strategy than in Lasso and Ridge. I will have to tune the training model over different ranges of $\lambda$ and $\alpha$. For this I can use the function _train_ and _trainControl_ from the _caret_ package. The reason I am using these functions is because they allow me flexibility to train the model with optimal tuning parameters of _lambda_ and _alpha_. Most tuning parameters for the function are chosen using the associated _R Documentation_ . 

For _trainControl_ , I tuned the function in the following way:

- method: _repeatedcv_ (for repeated training/test splits)
- number: _10 - 15_ number of folds or resampling iterations
- repeats: _5 - 10_ The number of complete sets of folds to compute - for repeated cv
- search: _random_, a random search procedure is used which describes howthe tuning parameter grid is determined (Bergstra and Bengio, 2012)
- verboseIter: _TRUE_, in order to print the training log

For _train_ I tuned the function in the following way:

- preProcess: preprocessing that has to be done on any test set (determined by looking at the training set). _center_ subtracts the mean of the predictor's data (again from the data in x) from the predictor values while _scale_ divides by the standard deviation.
- tuneLength: We choose _10_ , its the granularity of the tuning parameter grid. 
- trControl: _trainControl_ with the model training tuning parameters defined as above.

```{r,message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(caret)

tc <- trainControl(method="repeatedcv", number=10, repeats=5, search="random", 
                   verboseIter=FALSE)
#use training control in the elastic fit
dtrain <- data.frame(cbind(target,t,month))
fit_elastic <- train(target~.,data=dtrain,method="glmnet",
                     preProcess=c("center","scale"),tuneLength=10, trControl=tc)
fit_elastic$bestTune

```

Now that we have the values of $\alpha$ and $\lambda$ as `r round(fit_elastic$bestTune,3)` we use those values to fit the Elastic Net Regression model. 

```{r}
pred_elastic <- predict(fit_elastic, dtest)
ape_elastic <- sum((pred_elastic - nybirth_test)^2)/length(nybirth_test)
ape_elastic
```


So our final prediciton power with Ridge, Lasso and Elastic Net Regression is determined with their respective _validation APE_. 

- Ridge: `r ape_ridge`
- Lasso: `r ape_lasso`
- Elastic Net: `r ape_elastic`

Of the three, the one with the best prediction power is _Ridge Regression_ . In order to determine if the model can be used to make statistically significant inferences, I perform residual diagnostics for the model. 

```{r}
fit_ridge_train<- predict(fit_ridge, s=optimal_lambda_ridge, newx=t_bind)
resid_ridge <- as.vector(nybirth_train) - as.vector(fit_ridge_train)
fitted_ridge <- as.vector(fit_ridge_train)
par(mfrow=c(2,2))
# residuals vs. fitted values
plot(resid_ridge~fitted_ridge , pch=12)
abline(h=0,lwd=2,lty=2)
#QQplot
#qqnorm(resid_model)
#qqline(resid_model)
library(car)
qqPlot(resid_ridge , pch=12)
#Residuals plot
plot(resid_ridge , pch=12)
#ACF plot
acf(resid_ridge , pch=12)

#test constant variance
resid_seg <- rep(1:11, each=12)
fligner.test(resid_ridge, resid_seg)
#test Normality 
shapiro.test(resid_ridge)
#randomness test
library(lawstat)
runs.test(resid_ridge)
library(randtests)
difference.sign.test(resid_ridge)
```

Insights from the residuals plot:

- The residuals vs the fitted values indicates a bowl-shaped pattern and the residuals time series plot indicates a bowl-shaped pattern as well . This suggests that the mean is not constant 0
- The p-value in the runs test (5.841e-05) is very small, which confirms evidence against randomness of residuals. In the ACF plot, more than 5% of the spikes cross the confidence interval, indicating that the residuals are in fact dependant. 
- With the Fligner test we see that the p-value (0.475) is fairly large, hence there is no violation of the constant variance assumption. 
- From the QQ plot, we see that the upper right tail has significant outliers and the lower left tail of the distribution deviates slightly from the normal distribution, however the Shapiro test's p-value (0.03296) indicates that the residuals are not normal.

From the Residuals diagnostics, we see that the constant mean 0,independence and normality rules have been violated. However, the residuals have constant variance. Hence the _Ridge Regression_ model  , cannot be used to make statistically significant assumptions. This model will not be considered as a valid candidate in the model selection process. 


\newpage


## Holt-Winters Model 

I use the Exponential,Additive and Multiplicative Holt Winters Models to fit the data. 

```{r, message=FALSE, warning=FALSE}
library(stats)

#exponential data
hwfit_exp <- HoltWinters(nybirth_train, gamma=FALSE)
random_exp <- nybirth_train - hwfit_exp$fitted[,1]
sse_train_exp <- hwfit_exp$SSE
pred <- predict(hwfit_exp, n.ahead=36)
ape_validation_exp<-  sum((pred - nybirth_test)^2)/length(nybirth_test)
sse_train_exp
ape_validation_exp
par(mfrow=c(1,2))
plot(random_exp)
acf(random_exp)

#additive model
hwfit_additive <- HoltWinters(nybirth_train, seasonal='additive')
random_additive <- nybirth_train - hwfit_additive$fitted[,1]
sse_train_add <- hwfit_additive$SSE
pred1 <- predict(hwfit_additive, n.ahead = 36)
ape_validation_add <- sum((pred1 - nybirth_test)^2)/length(nybirth_test)
sse_train_add
ape_validation_add
par(mfrow=c(1,2))
plot(random_additive)
acf(random_additive)

#multiplicative model
hwfit_mult <- HoltWinters(nybirth_train, seasonal="multiplicative")
random_mult <- nybirth_train - hwfit_mult$fitted[,1]
sse_train_mult <- hwfit_mult$SSE
pred2 <- predict(hwfit_mult, n.ahead = 36)
ape_validation_mult <- sum((pred2 - nybirth_test)^2)/length(nybirth_test)
sse_train_mult
ape_validation_mult
par(mfrow=c(1,2))
plot(random_mult)
acf(random_mult)
```

We see that the Exponetial Model performs very poorly (significant trend in the residuals and dependancy as seen in the ACF) hence we eliminate it immediately and only discuss the other two. 

Insights from the Additive and Multiplicative Holt Winters models:

- There is no trend/seasonality from the plots (additive and multiplicative) hence the random components are stationary. 
- Since the importance of the model will be based on its ability to forecast the growth of the city, we test both the fit and prediction power of the model, but primarily focus on the prediction power.
- The fit with the multiplicative HW Model `r sse_train_mult` is slightly better than with the additive HW model `r sse_train_add` , however the predictive power of the additive model `r ape_validation_add` is much better that the multiplicative model's predictive power `r ape_validation_mult`.
- Hence of the two, I would choose the _Holt Winters Additive model_ for its predictive power.

In order to determine if the model can be used to make statistically significant inferences, I perform residual diagnostics for the model. 

```{r, message=FALSE, warning=FALSE}
fit_hw_train<- hwfit_additive$fitted[,1]
nytrain <- window(nybirth_ts, start= 1947, end=1956+11/12)
resid_hw <- as.vector(nytrain) - as.vector(fit_hw_train)
fitted_hw <- as.vector(fit_hw_train)
par(mfrow=c(2,2))
# residuals vs. fitted values
plot(resid_hw~fitted_hw , pch=12)
abline(h=0,lwd=2,lty=2)
#QQplot
#qqnorm(resid_model)
#qqline(resid_model)
library(car)
qqPlot(resid_hw , pch=12)
#Residuals plot
plot(resid_hw , pch=12)
#ACF plot
acf(resid_hw , pch=12)

#test constant variance
resid_seg <- rep(1:10, each=12)
fligner.test(resid_hw, resid_seg)
#test Normality 
shapiro.test(resid_hw)
#randomness test
library(lawstat)
runs.test(resid_hw)
library(randtests)
difference.sign.test(resid_hw)
```

Insights from the residuals plot:

- The residuals vs the fitted values is well scatters about the mean 0 and the residuals time series plot is well scattered too with no significant pattern . This suggests that the mean is a constant about 0.
- The p-value in the runs test (0.2713) is high, which confirms the hypothesis that the residuals are random. In the ACF plot, less than 5% of the spikes cross the confidence interval, indicating that the residuals are in fact independent. 
- With the Fligner test we see that the p-value (0.425) is fairly large, hence the constant variance assumption of residuals holds true.
- From the QQ plot, we see that the upper right tail has significant outliers and the lower left tail of the distribution deviates sligthly from the normal distribution, however the Shapiro test's p-value (0.1612) indicates that the residuals are normal.

From the Residuals diagnostics, we see that the constant mean 0,independence, constant variance, normality and randomness test have been fulfilled! Hence the _Holt Winters Additive_ model can be used to make statistically significant assumptions. This model will be considered as a valid candidate in the model selection process. 


\newpage

## Box Jenkins Model (ARIMA and/or SARIMA)
- ARIMA is represented by $(p,d,q)$
- SARIMA is represented by $(p,d,q)(P,D,Q){s}$

```{r}
par(mfrow=c(1,2))
plot(nybirth_train)
acf(nybirth_train, lag.max=36)
```

We observe that the data has both seasonality and a decreasing trend. The ACF plot also has a clear decreasing trend and periodic with a seasonal lag every 12 months. Hence seasonal and regular differencing is required. 

```{r, message=FALSE, warning=FALSE}
dif_train <- diff(nybirth_train)#differencing to remove linear trend
dif_train <- diff(dif_train, lag=12)#differencing to remove seasonality

plot(dif_train)
acf(dif_train, lag.max=46)
pacf(dif_train, lag.max=46)

seg <- c(rep(1:11, each=10), rep(12, 9))
flig1 <- fligner.test(dif_train,seg)
```

We also use the Fligner Test of homogeniety of variances and the differenced data has constant variance since the p value `r flig1$p.value` is the greater than 0.05. So the differenced data is stationary with _d = D =1 and s=12_. The SARIMA models that I propose are -

- Model 1: We first ignore the seasonal lags. The pacf undergoes exponential decay, the acf cuts off after lag 3, so _q=3_ and _p=0_. Focussing on the seasonal lags now, the acf cuts off after lag 1 , so _Q=1_ and _P=0_ . So one model can be $SARIMA(0,1,3)(0,1,1)_{12}$ 
- Model 2: We first ignore the seasonal lags. The acf undergoes exponential decay, and the pacf cuts off after lag 3, so _q=0_ and _p=3_. Focussing on the seasonal lags now, the pacf cuts off after lag 2, so _Q=0_ and _P=2_. So one model can be $SARIMA(3,1,0)(2,1,0)_{12}$
- Model 3: $SARIMA(0,1,3)(2,1,0)_{12}$
- Model 4: $SARIMA(3,1,0)(0,1,1)_{12}$ 

The other two combinations of models that originate from Model 1 and 2 are Model 3 and 4. Lets use these four models to evaluate their prediction power. 

```{r, message=FALSE, warning=FALSE}
library(astsa)
fit_sar1 <- sarima(nybirth_train,p=0,d=1,q=3,P=0,D=1,Q=1,S=12,Model = TRUE)
fit_sar2 <- sarima(nybirth_train,p=0,d=1,q=3,P=2,D=1,Q=0,S=12,Model = TRUE)
fit_sar3 <- sarima(nybirth_train,p=3,d=1,q=0,P=2,D=1,Q=0,S=12,Model = TRUE)
fit_sar4 <- sarima(nybirth_train,p=3,d=1,q=0,P=0,D=1,Q=1,S=12,Model = TRUE)
```
To see which of the models has the best prediction power, we look at the _PRESS_ statistic for models 1 - 4. 

```{r, message=FALSE, warning=FALSE}
par(mfrow=c(1,2))
pred_sar1 <- sarima.for(nybirth_train,n.ahead=36,p=0,d=1,q=3,P=0,D=1,Q=1,S=12)
pred_sar2 <- sarima.for(nybirth_train,n.ahead=36,p=0,d=1,q=3,P=2,D=1,Q=0,S=12)
par(mfrow=c(1,2))
pred_sar3 <- sarima.for(nybirth_train,n.ahead=36,p=3,d=1,q=0,P=2,D=1,Q=0,S=12)
pred_sar4 <- sarima.for(nybirth_train,n.ahead=36,p=3,d=1,q=0,P=0,D=1,Q=1,S=12)

APE_SAR =c()
APE_SAR[1]= sum((pred_sar1$pred - nybirth_test)^2)/length(nybirth_test)
APE_SAR[2]= sum((pred_sar2$pred - nybirth_test)^2)/length(nybirth_test)
APE_SAR[3]= sum((pred_sar3$pred - nybirth_test)^2)/length(nybirth_test)
APE_SAR[4]= sum((pred_sar4$pred - nybirth_test)^2)/length(nybirth_test)
APE_SAR
```
Based on the PRESS statistic, the best model is a choice between $SARIMA(0,1,3)(0,1,1)_{12}$ and $SARIMA(3,1,0)(0,1,1)_{12}$. We go ahead and assess the Residual Plot Diagnostic Tests for the two models that have been shown above when training the models. However, after observing the residuals plots for the two models, we confirm that the best one is $SARIMA(3,1,0)(0,1,1)_{12}$. The reason for this choice has been explained below.

- Model 1: $SARIMA(0,1,3)(0,1,1)_{12}$ 
  + There does not seem to be any pattern or variance instability in the Standardized Residuals Plot. 
  + There is no correlation seen in the ACF of the residuals, and we require the residuals to be _white noise_
  + Most points (except the right upper tail and left lower tail) follow the _Gaussian Distribution_
  + For the Ljung Box Statistic we would require all the points to lie above the dotted line, in this case quite a few touch the dotted line. 

- Model 2: $SARIMA(3,1,0)(0,1,1)_{12}$
  + There does not seem to be any pattern or variance instability in the Standardized Residuals Plot. 
  + There is no correlation seen in the ACF of the residuals, and we require the residuals to be _white noise_
  + Most points (except the right upper tail and left lower tail) follow the _Gaussian Distribution_
  + For the Ljung Box Statistic we would require all the points to lie above the dotted line, which is the case for this model.



\newpage

# Final Model

Generally models can be chosen based on their best fit, with values like _AIC, BIC and AICc_. To test the predictive power of the models in order to forecast the average births in New York, its best to choose the model with the lowest _APE_ that can be used to make statistically significant inferences. The two final candidates are the _Holt Winters Additive Model_ and $SARIMA(3,1,0)(0,1,1)_{12}$ since their residual diagnostic plots have been evaluated and fulfill the rules of residuals. When comparing the _APE_ we see that the predictive power of the $SARIMA(3,1,0)(0,1,1)_{12}$ (`r APE_SAR[4]`) is better than the _Holt Winters_ model (`r ape_validation_add`). Hence the final model is the $SARIMA(3,1,0)(0,1,1)_{12}$ . 

## Prediction

I use the $SARIMA(3,1,0)(0,1,1)_{12}$ to predict the average monthly births in NY for the years 1960 and 1961.

```{r, message=FALSE, warning=FALSE}
final_pred <- sarima.for(nybirth_ts,n.ahead=24,p=3,d=1,q=0,P=0,D=1,Q=1,S=12, plot.all=FALSE)
final_pred$pred
plot(nybirth_ts, xlim=c(1946,1965), ylim=c(20,32))
lines(final_pred$pred, col="red")
```


# Conclusion

The model-building process in this report highlights the trial and error and insights that develop with building simpler to advanced models for Time series data. It is imperative to note that both the Residual Diagnostics and the predictive power of a model determined by APE, play a significant role in choosing the best model to forecast the average births. The _SARIMA_ was the best model to use for forecasting with the NYBirth time series data but other models may have been chosen where fit is also concerned. 
